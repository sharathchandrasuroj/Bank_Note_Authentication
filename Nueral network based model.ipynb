{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/tanyabolla/Machine-Learning-Banknote-Authentication/blob/master/Banknote%20Authentication.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Clear the session\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the training and testing data\n",
    "ntr = 1000\n",
    "nts = 372\n",
    "Xtr, Xts, ytr, yts = train_test_split(X,y,train_size=ntr, test_size=nts,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the training and test matrices\n",
    "xmean = np.mean(Xtr, axis = 0)\n",
    "xstd = np.std(Xtr, axis = 0)\n",
    "Xtr_scale = (Xtr - xmean[None,:])/xstd[None, :]\n",
    "Xts_scale = (Xts - xmean[None,:])/xstd[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the model\n",
    "nin = Xtr.shape[1]\n",
    "nout = np.max(ytr) + 1\n",
    "nh = 256\n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(nin, ), activation='sigmoid', name= 'hidden'))\n",
    "model.add(Dense(nout, activation = 'softmax', name = 'output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "        self.loss = []\n",
    "        self.val_acc = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # TODO:  This is called at the end of each batch.  \n",
    "        # Add the loss in logs.get('loss') to the loss list\n",
    "        self.loss.append(logs.get('loss'))\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # TODO:  This is called at the end of each epoch.  \n",
    "        # Add the test accuracy in logs.get('val_acc') to the val_acc list\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "# Create an instance of the history callback\n",
    "history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = optimizers.Adam(lr = 0.001)\n",
    "model.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "model.fit(Xtr_scale, ytr, epochs = 10, batch_size=batch_size, validation_data=(Xts_scale, yts), callbacks=[history_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "val_acc = history_cb.val_acc\n",
    "epochs = len(val_acc)\n",
    "plt.plot(np.arange(1, epochs+1), val_acc, 'o-', linewidth = 2)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy of testing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss graph\n",
    "nsteps = len(history_cb.loss)\n",
    "ntr = Xtr.shape[0]\n",
    "epochs = np.arange(1, nsteps + 1)*batch_size/ntr\n",
    "plt.semilogy(epochs, history_cb.loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.xlim([0, np.max(epochs)])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Results:\n",
    "The accuracy of Neural net classifier is lower than the accuracy with Logistic regression.\n",
    "\n",
    "We believe it might be because of the learning rate. Next, we are going to try with a higher learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the session\n",
    "K.clear_session()\n",
    "\n",
    "#Construct the model\n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(nin, ), activation='sigmoid', name= 'hidden'))\n",
    "model.add(Dense(nout, activation = 'softmax', name = 'output'))\n",
    "\n",
    "#Optimizer\n",
    "opt = optimizers.Adam(lr = 0.01)\n",
    "model.compile(optimizer=opt, loss ='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100\n",
    "model.fit(Xtr_scale, ytr, epochs = 10, batch_size=batch_size, validation_data=(Xts_scale, yts), callbacks=[history_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "val_acc = history_cb.val_acc\n",
    "epochs = len(val_acc)\n",
    "plt.plot(np.arange(1, epochs+1), val_acc, 'o-', linewidth = 2)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy of testing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss graph\n",
    "nsteps = len(history_cb.loss)\n",
    "ntr = Xtr.shape[0]\n",
    "epochs = np.arange(1, nsteps + 1)*batch_size/ntr\n",
    "plt.semilogy(epochs, history_cb.loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.xlim([0, np.max(epochs)])\n",
    "plt.tight_layout()\n",
    "\n",
    "#Results: this loss function converges faster than the one because the learning rate is higher.\n",
    "#However, the accuracy of this learning is higher and more along the lines of Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With the learning rate of 0.01, the Neural net classifier gives an accuracy of 98% which is very close to the accuracy of the Logistic Regression (if you think about the precision of the testing data)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So, why did the learning rate have to increased?\n",
    "The learning rate is used to find the global minima. If the learning rate is small, the model reaches the global minima with better accuracy because it is slow. However, if it is too slow, we might end up reaching a local minima rather than the global minima (also depending on where you begin traversal). For instance, before we tried the learning rate of 0.01, we also tried 0.001, but the accuracy then was around 68% whch is quite low.\n",
    "\n",
    "Therefore, we concluded that the learning rate of 0.01 is the optimal learning rate.\n",
    "\n",
    "Furthermore, for this dataset, Logistic Regression is sufficient enough to provide accurate results, and a neural net is not necessary. However, it can help with understanding the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
